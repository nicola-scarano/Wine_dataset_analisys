{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0PKGTq0AzvT",
        "outputId": "de5dc346-0126-4d64-aa70-fc7126970e39"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "df_dev = pd.read_csv('./dev.tsv',sep='\\t')\r\n",
        "df_eval = pd.read_csv('./eval.tsv',sep='\\t')\r\n",
        "df_dev.drop_duplicates(inplace=True)\r\n",
        "len(df_dev), len(df_eval)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85028, 30186)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miMZUwhED3DD"
      },
      "source": [
        "#drop outliers\r\n",
        "df = pd.concat([df_dev, df_eval], sort=False, ignore_index=True)\r\n",
        "index = df[df['quality'] == 0].index\r\n",
        "df.drop(index = index, inplace = True)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCJMDjWJD55w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a70e83-3281-4ffd-ede4-75037baf7344"
      },
      "source": [
        "# substitution of province in region_1\r\n",
        "index_nan = df[df[\"region_1\"].isna()].index\r\n",
        "\r\n",
        "df.loc[index_nan, \"region_1\"] = df.loc[index_nan,\"province\"]\r\n",
        "# df.loc[index_nan, \"province\"] = df.loc[index_nan,\"country\"]\r\n",
        "\r\n",
        "index_nan = df[df[\"province\"].isna()].index\r\n",
        "df.drop(index = index_nan, inplace=True)\r\n",
        "train_valid_mask = ~df[\"quality\"].isna()\r\n",
        "df.shape"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115201, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmKVuufTEjl1"
      },
      "source": [
        "# REGION_1 CLEANING\r\n",
        "limit = 50\r\n",
        "df_1 = df\r\n",
        "for prov in df_1['province'].unique():\r\n",
        "  regions = df_1.loc[df_1['province'] == prov, 'region_1']\r\n",
        "  to_rename = pd.value_counts(regions)[pd.value_counts(regions) < limit]\r\n",
        "  for el in to_rename.index:\r\n",
        "    df_1.loc[df_1['region_1'] == el,'region_1'] = f'{prov} other'"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKM1_RgBLCxC"
      },
      "source": [
        "# REGION_1 CLEANING 2\r\n",
        "limit = 50\r\n",
        "df_1 = df\r\n",
        "for cou in df_1['country'].unique():\r\n",
        "  regions = df_1.loc[df_1['country'] == cou, 'region_1']\r\n",
        "  to_rename = pd.value_counts(regions)[pd.value_counts(regions) < limit]\r\n",
        "  for el in to_rename.index:\r\n",
        "    df_1.loc[df_1['region_1'] == el,'region_1'] = f'{cou} other'"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h-hnH2bLWfV"
      },
      "source": [
        "# VARIETY CLEANING\r\n",
        "limit = 20\r\n",
        "for var in df['variety'].unique():\r\n",
        "    if len(df[df['variety'] == var]) < limit:\r\n",
        "        df.loc[df['variety'] == var, 'variety'] = 'Variety other'\r\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoR2CuUTa44P"
      },
      "source": [
        "Preprocessing on winery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_yOUgZxQtl"
      },
      "source": [
        "# number of wine per winery\r\n",
        "winery_count = pd.value_counts(df[\"winery\"])\r\n",
        "df['winery_count'] = winery_count[df['winery']].values"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCZwWvzMZ96V"
      },
      "source": [
        "#number of variety produced per winery\r\n",
        "df['var_x_win'] = df.groupby(['winery']).variety.nunique()[df['winery']].values"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGnx4w1PYKeT"
      },
      "source": [
        "#number of wine of that variety produced by that winery\r\n",
        "# number of wine per variety produced by that winery \r\n",
        "wine_var_winery = {}\r\n",
        "for win in df['winery'].unique():\r\n",
        "  for var in df[df['winery'] == win]['variety'].unique():\r\n",
        "    wine_var_winery[f'{win}_{var}'] = len(df[ (df['winery'] == win) & (df['variety'] == var) ])\r\n",
        "\r\n",
        "win = []\r\n",
        "for el1,el2 in zip(df['winery'], df['variety']):\r\n",
        "  win.append(wine_var_winery[f'{el1}_{el2}'])\r\n",
        "df['wine_x_var_x_win'] = win\r\n",
        "df['ratio_variety_x_winery'] = df['wine_x_var_x_win']/df['winery_count']"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hkqNZj3eqbp"
      },
      "source": [
        "Preprocessing on designation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po6L8P4k_ooT",
        "outputId": "e05f2a34-94fc-4604-e323-631c26206ee3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "# import nltk\r\n",
        "# nltk.download()\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oaAAASZ_ooU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677e66fe-094f-4d67-c09b-f2cc1194e64a"
      },
      "source": [
        "class LemmaTokenizer(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "  def __call__(self, document):\r\n",
        "    lemmas = []\r\n",
        "    for t in word_tokenize(document):\r\n",
        "      t = t.strip()\r\n",
        "      lemma = self.lemmatizer.lemmatize(t)\r\n",
        "      lemmas.append(lemma)\r\n",
        "    return lemmas\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "\r\n",
        "train_valid_mask = ~df[\"quality\"].isna()\r\n",
        "\r\n",
        "lemmaTokenizer = LemmaTokenizer()\r\n",
        "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer, stop_words=stop_words, binary=True, use_idf=False, norm=False)\r\n",
        "# vectorizer_2 = TfidfVectorizer(tokenizer=lemmaTokenizer, stop_words=stop_words, binary=True, use_idf=True, norm=False)\r\n",
        "tfidf_train_desi = vectorizer.fit_transform(df.loc[train_valid_mask,'designation'].fillna(''))\r\n",
        "\r\n",
        "N = 120\r\n",
        "freq = sorted(zip(vectorizer.get_feature_names(), tfidf_train_desi.sum(axis=0).tolist()[0]), key=lambda x: x[1], reverse=True)[:N]\r\n",
        "freq \r\n",
        "\r\n",
        "tfidf_df = vectorizer.transform(df['designation'].fillna(''))\r\n",
        "\r\n",
        "# mask to be used to filter columns in wpm (only keeps the ones for the 100 most frequent words)\r\n",
        "words = [ word for word, _ in freq ]\r\n",
        "mask = [ w in words for w in vectorizer.get_feature_names()]\r\n",
        "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\r\n",
        "words_df1 = pd.DataFrame(data = tfidf_df[:, np.array(mask)].toarray(), columns=[f\"word_{word}\" for word in words_], index=df.index)\r\n",
        "\r\n",
        "\r\n",
        "df_word = df.join(words_df1)\r\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ePDGHxNZxN"
      },
      "source": [
        "One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8soyRPNSNYwN"
      },
      "source": [
        "from scipy import sparse\r\n",
        "# df_1 = pd.get_dummies(df_1, columns=['province','country', 'variety'], prefix = '', prefix_sep = '')\r\n",
        "df_1 = df_word\r\n",
        "\r\n",
        "sample_province = list(pd.value_counts(df_1[\"province\"])[-1:-356:-1].index)\r\n",
        "# sample_region_1 = list(pd.value_counts(df_1[\"region_1\"])[-1:-1482:-1].index)\r\n",
        "# sample_variety = list(pd.value_counts(df_1[\"variety\"])[-1:-532:-1].index)\r\n",
        "# sample_country = list(pd.value_counts(df_1[\"country\"])[-1:-34:-1].index)\r\n",
        "# sample_winery = list(pd.value_counts(df_1[\"winery\"])[-1:-8809:-1].index) \r\n",
        "# sample_winery = ['winery'+win for win in sample_winery]\r\n",
        "\r\n",
        "df_1 = pd.get_dummies(df_1, columns= ['region_1', 'region_2', 'variety', 'province'],prefix='', prefix_sep = '')\r\n",
        "\r\n",
        "df_1.drop(columns = sample_province, inplace = True)\r\n",
        "# df_1.drop(columns = sample_region_1, inplace = True, errors='ignore')\r\n",
        "# df_1.drop(columns = sample_variety, inplace = True, errors='ignore')\r\n",
        "# df_1.drop(columns = sample_winery, inplace = True, errors='ignore')"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BMN7kzCpuCe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "df_dropped = df_1.drop(columns = ['designation', 'country', 'winery','description'])\r\n",
        "train_valid_mask = ~df_dropped[\"quality\"].isna()\r\n",
        "# extract the feature names (for later use)\r\n",
        "feature_names = df_dropped[train_valid_mask].drop(columns=[\"quality\"]).columns\r\n",
        "X = df_dropped.drop(columns=[\"quality\"]).values\r\n",
        "y = df_dropped[\"quality\"].values\r\n",
        "X_train_valid = X[train_valid_mask]\r\n",
        "y_train_valid = y[train_valid_mask]\r\n",
        "X_test = X[~train_valid_mask]\r\n",
        "y_test = y[~train_valid_mask]\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid,y_train_valid, shuffle=True, random_state=42)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQeHJMMGp76m"
      },
      "source": [
        "reg = RandomForestRegressor(150, random_state=42)\r\n",
        "reg.fit(X_train , y_train)\r\n",
        "r2_score(y_valid, reg.predict(X_valid))"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVaF1GkURpT"
      },
      "source": [
        "#Prediction on test set\r\n",
        "reg = RandomForestRegressor(n_estimators=150, random_state=42)\r\n",
        "reg.fit(X_train_valid , y_train_valid)\r\n",
        "y_pred = reg.predict(X_test)\r\n",
        "pd.DataFrame(y_pred, index=df_eval.index).to_csv(\"output.csv\",index_label=\"Id\", header=[\"Predicted\"])"
      ],
      "execution_count": 193,
      "outputs": []
    }
  ]
}